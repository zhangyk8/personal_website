<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>STAT 302 Statistical Computing</title>
    <meta charset="utf-8" />
    <meta name="author" content="Yikun Zhang (Autumn 2023)" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="uw.css" type="text/css" />
    <link rel="stylesheet" href="fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, top, title-slide

.title[
# STAT 302 Statistical Computing
]
.subtitle[
## Lecture 8: Numerical Analysis
]
.author[
### Yikun Zhang (<em>Autumn 2023</em>)
]

---




# Outline

1. Mathematical Preliminaries (Differentiation and Integration)

2. Computer Arithmetic and Error Analysis

3. Algorithmic Rate of Convergence

4. Solutions of Equations in One Variable

5. Numerical Integration

&lt;p&gt;&lt;font size="4"&gt;* Acknowledgement: Parts of the slides are modified from the course materials by Prof. Deborah Nolan. &lt;/font&gt;&lt;/p&gt;

&lt;p&gt;&lt;font size="4"&gt;* Reference: Numerical Analysis (9th Edition) by Richard L. Burden, J. Douglas Faires, Annette M. Burden, 2015. &lt;/font&gt;&lt;/p&gt;

---
class: inverse

# Part 1: Mathematical Preliminaries (Differentiation and Integration)

---

# Limits and Continuity of A Function

A function `\(f\)` defined on a set `\(\mathcal{X}\)` of real numbers has the **limit** `\(L\)` at `\(x_0\in \mathcal{X}\)`, written `\(\lim\limits_{x\to x_0} f(x) = L\)`, if, given any `\(\epsilon &gt;0\)`, there exists a real number `\(\delta&gt;0\)` such that 
`$$|f(x)-L| &lt; \epsilon \quad \text{ whenever } \quad x\in \mathcal{X} \quad \text{ and } \quad 0&lt; |x-x_0|  &lt; \delta.$$`

&lt;p align="center"&gt;
&lt;img src="./figures/function_limit.png" width="530"/&gt;
&lt;/p&gt;

--

- Then, `\(f\)` is **continuous** at `\(x_0\)` if `\(\lim\limits_{x\to x_0} f(x) = f(x_0)\)`.
- `\(f\)` is **continuous on the set `\(\mathcal{X}\)`** if it is continuous at each point in `\(\mathcal{X}\)`. We denote it by `\(f\in \mathcal{C}(\mathcal{X})\)`.

---

# Limits and Continuity of A Function

Let `\(\{x_n\}_{n=1}^{\infty}\)` be an infinite sequence of real numbers. This sequence has the **limit** `\(x\)` (i.e., **converges to** `\(x\)`) if, for any `\(\epsilon&gt;0\)`, there exists a positive integer `\(N(\epsilon)\)` such that 
`$$|x_n -x| &lt; \epsilon \quad \text{ whenever } \quad n&gt;N(\epsilon).$$`
We denote it by `\(\lim\limits_{n\to\infty} x_n =x\)`.

--

**Theorem.** If `\(f\)` is a function defined on a set `\(\mathcal{X}\)` and `\(x_0 \in \mathcal{X}\)`, the the following statements are equivalent:

a. `\(f\)` is continuous at `\(x_0\)`;

b. If `\(\{x_n\}_{n=1}^{\infty}\)` is any sequence in `\(\mathcal{X}\)` converging to `\(x_0\)`, then 
`$$\lim\limits_{n\to\infty} f(x_n) = f(x_0).$$`

---

# Differentiability of A Function

Let `\(f\)` be a function defined in an open set containing `\(x_0\)`. The function `\(f\)` is **differentiable** at `\(x_0\)` if 
`$$f'(x_0) = \lim_{x\to x_0} \frac{f(x)-f(x_0)}{x-x_0}$$`
exists. The number `\(f'(x)\)` is called the **derivative** of `\(f\)` at `\(x_0\)`. A function that has a derivative at each point in a set `\(\mathcal{X}\)` is **differentiable on** `\(\mathcal{X}\)`.

--

- The derivative of `\(f\)` at `\(x_0\)` is the slope of the tangent line to the graph of `\(f\)` at `\((x_0,f(x_0))\)`.

&lt;p align="center"&gt;
&lt;img src="./figures/function_deriv.png" width="530"/&gt;
&lt;/p&gt;

---

# Taylor's Theorem

Assume that `\(f\)` is `\(n\)`-times continuously differentiable and `\(f^{(n+1)}\)` exists on `\(x_0\in [a,b]\)`. For every `\(x\in [a,b]\)`, there exists a number `\(\xi(x)\)` between `\(x_0\)` and `\(x\)` such that
`\begin{align*}
f(x) &amp;= P_n(x) + R_n(x)\\
&amp;= \underbrace{\sum_{k=0}^n \frac{f^{(k)}(x_0)}{k!}\cdot (x-x_0)^k}_{n^{th} \text{ Taylor's polynomial}} + \underbrace{\frac{f^{(n+1)}(\xi(x))}{(n+1)!} \cdot (x-x_0)^{n+1}}_{\text{Remainder term or Truncation error}}.
\end{align*}`

--

- As `\(n\to \infty\)`, `\(\lim\limits_{n\to\infty} P_n(x)\)` becomes an infinite Taylor series of `\(f\)` at around the point `\(x_0\)`.

---

# Theorems Related to Differentiability

**Theorem.** If the function `\(f\)` is differentiable at `\(x_0\)`, then `\(f\)` is continuous at `\(x_0\)`.

--

**Rolle's Theorem.** Suppose that `\(f\in \mathcal{C}[a,b]\)` and `\(f\)` is differentiable on `\((a,b)\)`. If `\(f(a) = f(b)\)`, then there exists a number `\(c\in (a,b)\)` such that `\(f'(c)=0\)`.

&lt;p align="center"&gt;
&lt;img src="./figures/rolle_theorem.png" width="700"/&gt;
&lt;/p&gt;

---

# Theorems Related to Differentiability

**Mean Value Theorem.** If `\(f\in \mathcal{C}[a,b]\)` and `\(f\)` is differentiable on `\((a,b)\)`, then there exists a number `\(c\in (a,b)\)` such that 
`$$f'(c)=\frac{f(b) -f(a)}{b-a}.$$`

&lt;p align="center"&gt;
&lt;img src="./figures/Mean_Value_Theorem.png" width="700"/&gt;
&lt;/p&gt;

---

# Theorems Related to Differentiability

**Extreme Value Theorem.** If `\(f\in \mathcal{C}[a,b]\)`, then there exist `\(c_1,c_2 \in [a,b]\)` such that
`$$f(c_1) \leq f(x) \leq f(c_2) \quad \text{ for all } \quad x\in [a,b].$$`
In addition, if `\(f\)` is differentiable on `\((a,b)\)`, then the numbers `\(c_1\)` and `\(c_2\)` occur either at the endpoints of `\([a,b]\)` or at the point where `\(f'\)` is zero.

&lt;p align="center"&gt;
&lt;img src="./figures/Extreme_Value_Theorem.png" width="700"/&gt;
&lt;/p&gt;

---

# Other Generalized Theorems

**Generalized Rolle's Theorem.** Suppose that `\(f \in \mathcal{C}[a,b]\)` is `\(n\)` times differentiable on `\((a,b)\)`. If `\(f(x)=0\)` at the `\((n+1)\)` distinct numbers `\(a\leq x_0 &lt; x_1 &lt; \cdots &lt; x_n\leq b\)`, then there exists a number `\(c\in (x_0,x_n)\)` such that `\(f^{(n)}(c)=0\)`.

--

**Intermediate Value Theorem.** If `\(f\in \mathcal{C}[a,b]\)` and `\(K\)` is any number between `\(f(a)\)` and `\(f(b)\)`, then there exists a number `\(c\in (a,b)\)` for which `\(f(c)=K\)`.

&lt;p align="center"&gt;
&lt;img src="./figures/Inter_mean_value.png" width="650"/&gt;
&lt;/p&gt;

---

# Compute Derivatives in R

We can use the build-in function `D()` in R to compute the derivative of a function.


```r
f = expression(x^2 + 3*x)
class(f)
```

```
## [1] "expression"
```

--


```r
# First-order derivative
D(f, 'x')
```

```
## 2 * x + 3
```

```r
# Second-order and third-order derivatives
D(D(f, 'x'), 'x')
```

```
## [1] 2
```

```r
D(D(D(f, 'x'), 'x'), 'x')
```

```
## [1] 0
```

---

# Compute Derivatives in R

It is _inconvenient_ to compute the high-order derivatives by using `D()` in a nested way.

--

- Instead, we can write our customized function in a recursive way to compute the high-order derivative.


```r
compDeriv = function(expr, name, order = 1) {
   if(order &lt; 1) stop("'order' must be &gt;= 1")
   if(order == 1) {
     return(D(expr, name))
   } else {
     return(compDeriv(D(expr, name), name, order - 1))
   }
}
exp1 = expression(x^3)
compDeriv(exp1, 'x', 3)
```

```
## 3 * 2
```

Note: The process where a function calls itself directly or indirectly is called _recursion_, and the corresponding function is called a recursive function. 

---

# Compute Derivatives in R

If the expression has more than one independent variable, we can calculate the partial derivative with respect to each of them.


```r
g = expression(x^2 + y^3 + 2*x*y - 3*x + 4*y + 4)
D(g, 'x')
```

```
## 2 * x + 2 * y - 3
```

```r
D(g, 'y')
```

```
## 3 * y^2 + 2 * x + 4
```

```r
compDeriv(g, 'x', 2)
```

```
## [1] 2
```

```r
compDeriv(g, 'y', 3)
```

```
## 3 * 2
```

---

# Evaluate R Expression and its Derivatives

We can evaluate the function expression and its derivative using the build-in function `eval()`.


```r
x = 1:5
f = expression(x^2 + 3*x)
eval(f)
```

```
## [1]  4 10 18 28 40
```
--


```r
df = D(f, 'x')
class(df)
```

```
## [1] "call"
```

```r
df(1:3)
```

```
## Error in df(1:3): argument "df1" is missing, with no default
```

```r
eval(D(f, 'x'))
```

```
## [1]  5  7  9 11 13
```

---

# Evaluate R Expression and its Derivatives

We can evaluate the function expression and its derivative using the build-in function `eval()`.


```r
g = expression(x^2 + y^3 + 2*x*y - 3*x + 4*y + 4)
x = 1:5
y = 3:7
eval(g)
```

```
## [1]  47  98 179 296 455
```

```r
eval(compDeriv(g, 'y', 2))
```

```
## [1] 18 24 30 36 42
```

```r
eval(compDeriv(g, 'x', 2))
```

```
## [1] 2
```

---

# Evaluate R Expression and its Derivatives

Using the `eval()` function requires us to assign correct values to the R variables as those in our expression.


```r
remove(x)
eval(f)
```

```
## Error in eval(f): object 'x' not found
```

--

Alternatively, we can use the build-in function `deriv()` to compute the derivatives of a function.

- The return value of `deriv()` with argument `function.arg = TRUE` is an R function!


```r
g = expression(x^2 + y^3 + 2*x*y - 3*x + 4*y + 4)
dx = deriv(g, 'x', function.arg = TRUE)
dxy = deriv(g, c('x', 'y'), function.arg = TRUE)
class(dx)
```

```
## [1] "function"
```

---

# Evaluate R Expression and its Derivatives


```r
D(g, 'x')
```

```
## 2 * x + 2 * y - 3
```

```r
y = 4:8
dx_res = dx(10)
dx_res
```

```
## [1] 234 319 434 585 778
## attr(,"gradient")
##       x
## [1,] 25
## [2,] 27
## [3,] 29
## [4,] 31
## [5,] 33
```

```r
dx_res[1]
```

```
## [1] 234
```

---

# Evaluate R Expression and its Derivatives

We can access the "gradient" attribute using the `attr()` function in R.


```r
attr(dx_res, "gradient")
```

```
##       x
## [1,] 25
## [2,] 27
## [3,] 29
## [4,] 31
## [5,] 33
```

```r
dxy(1:3, 3:5)
```

```
## [1]  47  98 179
## attr(,"gradient")
##       x  y
## [1,]  5 33
## [2,]  9 56
## [3,] 13 85
```


---

# Integration

The Riemann integral of the function `\(f\)` on the interval `\([a,b]\)` is the following limit, provided it exists:
`$$\int_a^b f(x) dx = \lim_{\max \Delta x_i \to 0} \sum_{i=1}^n f(z_i) \Delta x_i,$$`
where the numbers `\(x_0,x_1,...,x_n\)` satisfy `\(a=x_0\leq x_1 \leq \cdots \leq x_n=b\)`, where `\(\Delta x_i = x_i-x_{i-1}\)` for each `\(i=1,2,...,n\)` and `\(z_i\)` is an arbitrary number in the interval `\([x_{i-1}, x_i]\)`.

&lt;p align="center"&gt;
&lt;img src="./figures/riem_inte.png" width="600"/&gt;
&lt;/p&gt;

---

# Fundamental Theorem of Calculus

- If `\(f\in \mathcal{C}[a,b]\)` and we define `\(F(x)=\int_a^x f(t) dt\)`, then `\(F'(x)=f(x)\)` for any `\(x\in (a,b)\)`. Hence, `\(F\)` is an anti-derivative of `\(f\)`.

- If `\(f\in \mathcal{C}[a,b]\)` and `\(F\)` is any anti-derivative of `\(f\)`, then 
`$$\int_a^b f(x) dx = F(x)\Big|_{x=a}^b = F(b) - F(a).$$`

&lt;p align="center"&gt;
&lt;img src="./figures/Fundamental_theorem_of_calculus.gif" width="450"/&gt;
&lt;/p&gt;

---

# Weighted Mean Value Theorem for Integrals

Suppose that `\(f\in \mathcal{C}[a,b]\)`, the Riemann integral of `\(g\)` exists on `\([a,b]\)`, and `\(g(x)\)` does not change sign on `\([a,b]\)`. Then, there exists a number `\(c\in (a,b)\)` such that
`$$\int_a^b f(x) g(x)\, dx = f(c) \int_a^b g(x)\, dx.$$`

- When `\(g(x)\equiv 1\)`, it leads to the usual Mean Value Theorem for integrals: `\(f(c) = \frac{1}{b-a} \int_a^b f(x)\, dx\)`.

&lt;p align="center"&gt;
&lt;img src="./figures/weight_mean_value.png" width="500"/&gt;
&lt;/p&gt;

---

# Compute Integrals in R

A Riemann integral can be computed via a build-in function `integrate()` in R.


```r
inteFun1 = function(x) { x^2 + 3*x }
integrate(inteFun1, lower = 0, upper = 2)
```

```
## 8.666667 with absolute error &lt; 9.6e-14
```

```r
# An integral that converges slowly
inteFun2 = function(x) {
  return(1/((x+1)*sqrt(x)))
}
integrate(inteFun2, lower = 0, upper = Inf)
```

```
## 3.141593 with absolute error &lt; 2.7e-05
```

--


```r
integrate(inteFun1, lower = 0, upper = Inf)
```

```
## Error in integrate(inteFun1, lower = 0, upper = Inf): the integral is probably divergent
```

---

# Compute Multiple Integrals in R

Compute a multiple integral `\(\int_0^1 \int_0^2 \int_0^{\frac{1}{2}} (xy + z^2 + 3yz) \,dxdydz\)`.


```r
library(cubature)
mulFun1 = function(x) { 
  return(x[1]*x[2] + x[3]^2 + 3*x[2]*x[3])
}
# "x" is a vector and x[1], x[2], x[3] refers to x, y, and z, respectively.
mul_int = adaptIntegrate(mulFun1, lowerLimit = c(0, 0, 0), upperLimit = c(0.5, 2, 1))
mul_int
```

```
## $integral
## [1] 2.083333
## 
## $error
## [1] 4.440892e-16
## 
## $functionEvaluations
## [1] 33
## 
## $returnCode
## [1] 0
```

---
class: inverse

# Part 2: Computer Arithmetic and Error Analysis

---

# Numbers and Characters in Computer Systems

How does our computer store and recognize numbers and characters?

--

- It encodes every number/character as a sequence of **bits** (i.e., binary digits 0 or 1).

- It was [John Tukey](https://en.wikipedia.org/wiki/John_Tukey), a prestigious statistician, who first coined the term "bit".

- A byte is a collection of 8 bits, e.g., 0001 0011.

---

# Character Encoding Systems

There are several standard encoding systems with bits for characters. 

- [ASCII](https://en.wikipedia.org/wiki/ASCII) -- American Standard Code for Information Interchange. 

  - Each upper and lower case letter in the English alphabet and other characters is represented as a sequence of 7 bits.

&lt;p align="center"&gt;
&lt;img src="./figures/USASCII_code_chart.png" width="500"/&gt;
&lt;/p&gt;

---

# Character Encoding Systems

There are several standard encoding systems with bits for characters. 

- Nowadays, [Unicode](https://en.wikipedia.org/wiki/Unicode) is more commonly used for text encoding, including [UTF-8](https://en.wikipedia.org/wiki/UTF-8), UTF-16, etc.

  - It encodes not just English letters but also special characters and emojis.

&lt;p align="center"&gt;
&lt;img src="./figures/unicode.jpg" width="550"/&gt;
&lt;/p&gt;

---

# Binary Number Representations

Commonly, we write a number in the base-10 decimal system.

- For instance, we write a 3-digit number **105**, representing 1 hundred, 0 tens, and 5 ones.

- That is, `\(105= 1\times 10^2 + 0 \times 10^1 + 5\times 10^0\)`.

--

In our computer systems, however, 105 is stored as **110 1001**, because
`\begin{align*}
105 &amp;= (1\times 2^6) + (1\times 2^5) + (0\times 2^4) \\
&amp;\quad + (1\times 2^3) + (0\times 2^2) + (0\times 2^1) + (1\times 2^0).
\end{align*}`

- What is the base-10 decimal value of the 8-digit binary number **0011 0011**?

--

- Answer: `\(1\times 2^0 + 1\times 2^1+ 1\times 2^4+ 1\times 2^5=51\)`.

---

# Scientific Notations

Integer types can be stored in our computer using the binary representation.

How about other numeric types, e.g., 0.25, 1/7, `\(\pi\)`, etc?

--

- The computer system uses the notion of **scientific notation** to store numbers.

--

- A general scientific notation in base-10 is written as: 
`$$a\times 10^{b}.$$`

  - Terminology: `\(\,a\)` --&gt; mantissa; `\(\quad \; b\)` --&gt; exponent.

  - Examples: `\(\,0.023\)`	--&gt; `\(2.3 \times 10^{-2}\)`; `\(\quad\)` `\(-2600\)` --&gt; `\(-2.6\times 10^3\)`.

Note: The computer cannot store 1/7 and `\(\pi\)` as their exact values because the total number of digits is limited.

---

# Double-Precision Floating Point

IEEE (Institute for Electrical and Electronic Engineers) in 1985 (updated in 2008) published a report to standardize 

- Binary and decimal floating numbers;

- Formats for data interchange;

- Algorithms for rounding arithmetic operations;

- Handling exceptions.

--

A 64-bit (binary digit) representation is used for a real number (i.e., the numeric data type in R).

&lt;p align="center"&gt;
&lt;img src="./figures/ieee_float.png" width="800"/&gt;
&lt;/p&gt;

---

# Double-Precision Floating Point

&lt;p align="center"&gt;
&lt;img src="./figures/ieee_float2.png" width="800"/&gt;
&lt;/p&gt;

Under base-2 system, 

- the first bit `\(s\)` is a sign indicator;

- it is followed by an 11-bit exponent (or characteristic) `\(c\)`;

- it is ended with a 52-bit binary fraction (or mantissa) `\(f\)`.

The represented number in the base-10 decimal system is
`$$(-1)^s\cdot 2^{c-1023}\cdot (1+f).$$`

---

# Double-Precision Floating Point

The smallest normalized positive number that can be represented has `\(s=0\)`, `\(c=1\)`, and `\(f=0\)` and is equivalent to
`$$2^{-1022} \cdot (1+0) \approx 2.2251 \times 10^{-308},$$`
and the largest has `\(s=0\)`, `\(c=2046\)`, and `\(f=1-2^{-52}\)` and is equivalent to
`$$2^{1023} \cdot (2- 2^{-52}) \approx 1.7977 \times 10^{308}.$$`


```r
.Machine$double.xmin
```

```
## [1] 2.225074e-308
```

```r
.Machine$double.xmax
```

```
## [1] 1.797693e+308
```

---

# Underflow and Overflow

- Numbers occurring in calculations that have a magnitude less than `\(2^{-1022} \cdot (1+0)\)` 
result in **underflow** and are generally set to 0.

- Numbers greater than `\(2^{1023} \cdot (2-2^{-52})\)` result in **overflow** and typically cause the computations to stop. In R, it will be denoted by `Inf`.


Note: Recall that
`$$(-1)^s\cdot 2^{c-1023}\cdot (1+f).$$`
There are two representations for the
number 0.

- A positive 0 when `\(s = 0\)`, `\(c = 0\)`, and `\(f = 0\)`; 

- A negative 0 when `\(s = 1\)`, `\(c = 0\)`, and `\(f = 0\)`.

---

# Decimal Machine Numbers

The use of binary digits may conceal the computational difficulties that occur when a finite collection of machine numbers is used to represent all the real numbers.

--

- For simplicity, we use base-10 decimal numbers in our analysis:
`$$\pm 0.d_1d_2\cdots d_k \times 10^n \quad \text{with}\quad 1\leq d_1 \leq 9 \; \text{ and } \; 0\leq d_i \leq 9 \;\text{ for }\; i=2,...,k.$$`
This is called `\(k\)`-digit _decimal machine numbers_.

- Any positive real number (within the numerical range of the machine) can be written as:
`$$y=0.d_1d_2\cdots d_kd_{k+1} d_{k+2} \cdots \times 10^n.$$`

---

# Decimal Machine Numbers

- There are two ways to represent `\(y\)` with `\(k\)` digits:
  
  - _Chopping_: chop off after `\(k\)` digits:
  `$$fl(y)=0.d_1d_2\cdots d_k \times 10^n.$$`

--

  - _Rounding_: Add `\(5\times 10^{n-(k+1)}\)` and chop:
  `$$fl(y)=0.\delta_1\delta_2\cdots \delta_k \times 10^n.$$`

Note: We make two remarks for rounding.

- When `\(d_{k+1} \geq 5\)`, we add `\(1\)` to the digit `\(d_k\)` to obtain the final floating point number `\(fl(y)\)`, i.e., we _round up_.

- When `\(d_{k+1} &lt; 5\)`, we simply chopp off all but the first `\(k\)` digits, i.e., we _round down_.

---

# Absolute and Relative Errors

Suppose that `\(\hat{p}\)` is an approximation (or rounding number) to `\(p^*\)`.

- The **absolute error** is `\(|\hat{p} -p^*|\)`.

- The **relative error** is `\(\frac{|\hat{p} -p^*|}{|p^*|}\)`, provided that `\(p^*\neq 0\)`.

--


```r
p_star = pi
p_hat = round(p_star, digits = 7)
cat("The absolute error is ", abs(p_star - p_hat), ".", sep = "")
```

```
## The absolute error is 4.641021e-08.
```

```r
cat("The relative error is ", abs(p_star - p_hat)/abs(p_star), ".", sep = "")
```

```
## The relative error is 1.477283e-08.
```

---

# Significant Digits

The number `\(\hat{p}\)` is said to approximate `\(p^*\)` to `\(t\)` **significant digits** (or figures) if `\(t\)` is the largest nonnegative integer for which
`$$\frac{|\hat{p} -p^*|}{|p^*|} \leq 5\times 10^{-t}.$$`

--


```r
sigDigit = function(x_app, x_true) {
  t = 0
  while(abs(x_app - x_true)/abs(x_true) &lt;= 5*10^(-t)) {
    t = t + 1
  }
  return(t)
}
p_star = pi
p_hat = round(p_star, digits = 7)
sigDigit(p_hat, p_star)
```

```
## [1] 9
```

---

# Floating Point Operations in Computers

Assume that the floating-point representations `\(fl(x)\)` and `\(fl(y)\)` are given for the real numbers `\(x\)` and `\(y\)`. Let `\(\oplus, \ominus, \otimes, \oslash\)` represent machine addition, subtraction, multiplication, and division operations.

--

- Then, the finite-digit arithmetics for floating points are
`\begin{align*}
x \oplus y &amp;= fl\left(fl(x) + fl(y) \right), \quad x\otimes y = fl\left(fl(x) \times fl(y) \right),\\
x \ominus y &amp;= fl\left(fl(x) - fl(y) \right), \quad x\oslash y = fl\left(fl(x) / fl(y) \right).
\end{align*}`

- "Round input, perform exact arithmetic, and round the result".

--


```r
x = round(1/3, digits = 4)
y = round(5/7, digits = 4)
print(1/3 + 5/7)
```

```
## [1] 1.047619
```

```r
# 3-digit arithmetic
round(x + y, digits = 4)
```

```
## [1] 1.0476
```

---

# Error Growth and Stability

Those floating point rounding errors or other systematic errors can propagate as our computations/algorithms proceed.

--

Suppose that `\(E_0 &gt;0\)` denotes an initial error at some stage of the calculations and `\(E_n\)` represents the magnitude of the error after `\(n\)` subsequent operations.

- If `\(E_n \approx C \cdot nE_0\)`, then the growth of error is said to be **linear**.

- If `\(E_n \approx C^n E_0\)` for some `\(C&gt;1\)`, then the growth of error is called **exponential**.

Note: Here, `\(C&gt;0\)` is a constant independent of `\(n\)`.

---

# Error Growth and Stability

- **Stable** algorithm: Small changes in the initial data only produce small changes in the final result.

- **Unstable** or **Conditionally Stable** algorithm: Large errors in the final result for all or some initial data with small changes.

&lt;p align="center"&gt;
&lt;img src="./figures/error_growth.png" width="700"/&gt;
&lt;/p&gt;

---
class: inverse

# Part 3: Algorithmic Rate of Convergence

---

# Rate/Order of Convergence

Suppose that `\(\{x_n\}_{n=0}^{\infty} \subset \mathbb{R}^d\)` converges to `\(x^*\in \mathbb{R}^d\)` and `\(\{\alpha_n\}_{n=0}^{\infty} \subset \mathbb{R}\)` is a sequence known to converge to 0.

--

- If there exists a constant `\(C&gt;0\)` independent of `\(n\)` with
`$$\|x_n -x^* \|_2 \leq C |\alpha_n| \quad \text{ for large } n,$$`
then we say that `\(\{x_n\}_{n=0}^{\infty}\)` converges to `\(x^*\)` with **rate, or order, of convergence** `\(O(\alpha_n)\)`.

--

- We can write 
`$$\|x_n-x^*\|_2 = O(\alpha_n)$$` 
or `\(x_n=x^* + O(\alpha_n)\)` when `\(\{x_n\}_{n=0}^{\infty}\)` is a sequence of numbers.

--

- In many cases, we can take `\(\alpha_n = \frac{1}{n^p}\)` for some number `\(p&gt;0\)`, and generally, the _largest_ value of `\(p\)` with `\(\|x_n-x^*\|_2= O\left(\frac{1}{n^p} \right)\)` will be of interest.

---

# Linear Convergence

When `\(\|x_n-x^*\|_2= O\left(\frac{1}{n^p} \right)\)` for the largest possible value of `\(p\)`, `\(x_n\)` is also known to converge to `\(x^*\)` in a polynomial/sublinear rate of convergence.

- The algorithm or sequence that polynomially/sublinearly converges to `\(x^*\)` is relatively _slow_, in the sense that
`$$\lim_{n\to\infty} \frac{\|x_{n+1}-x^*\|_2}{\|x_n -x^*\|_2}=1.$$`

--

The convergence `\(x_n\to x^*\)` is said to be **linear** if there exists a number `\(0&lt; r&lt; 1\)` such that
`$$\lim_{n\to\infty} \frac{\|x_{n+1}-x^*\|_2}{\|x_n -x^*\|_2}= r.$$`

- The **linear convergence** means that 
`$$\|x_n -x^*\|_2= r^n \|x_0 -x^*\| = O(r^n).$$`

---

# Q-Linear and R-Linear Convergences

The linear convergence can be further categorized into two types:

- Q-Linear Convergence: `\(\frac{\|x_{n+1}-x^*\|_2}{\|x_n -x^*\|_2}\leq r\)` for some `\(r\in (0,1)\)` when `\(n\)` is sufficiently large. This is the same as our previous definition.

--

- R-Linear Convergence of `\(\{x_n\}_{n=0}^{\infty} \subset \mathbb{R}^d\)`: there exists a sequence `\(\{\epsilon_n\}_{n=0}^{\infty}\)` such that
`$$\|x_n-x^*\|_2 \leq \epsilon_n \quad \text{ for all } n$$`
and `\(\epsilon_n\)` converges Q-linearly to 0.

- The "Q" stands for "quotient", while the "R" stands for "root".

--

- Examples: 
  - `\(\left\{\frac{1}{2^n} \right\}_{n=0}^{\infty}\)` converges Q-linearly to 0.
  - `\(\left\{1,1, \frac{1}{4},\frac{1}{4},\frac{1}{16},\frac{1}{16},...,\frac{1}{4^{\left\lfloor \frac{k}{2} \right\rfloor}},... \right\}\)` converges R-linearly to 0.

---

# Superlinear Convergence

The convergence `\(x_n\to x^*\)` is said to be **superlinear** if 
`$$\lim_{n\to\infty} \frac{\|x_{n+1}-x^*\|_2}{\|x_n -x^*\|_2}= 0.$$`

--

- Quadratic Convergence: We say that `\(\{x_n\}_{n=0}^{\infty}\)` converges quadratically to `\(x^*\)` if there exists a constant `\(M&gt;0\)` such that
`$$\lim_{n\to\infty} \frac{\|x_{n+1}-x^*\|_2}{\|x_n -x^*\|_2^2}= M.$$`

- Example: `\(\left\{\left(\frac{1}{n} \right)^{2^n} \right\}_{n=0}^{\infty}\)` converges quadratically to 0, because
`$$\frac{\|x_{n+1}-x^*\|_2}{\|x_n -x^*\|_2^2} = \frac{\left(\frac{1}{n+1} \right)^{2^{n+1}}}{\left(\frac{1}{n} \right)^{(2^n)\cdot 2}} = \left(\frac{n}{n+1} \right)^{2^{n+1}}\leq 1.$$`

---

# Plots for Different Types of Convergence

&lt;p align="center"&gt;
&lt;img src="./figures/conv_plot.png" width="650"/&gt;
&lt;/p&gt;

---

# Plots for Different Types of Convergence


```r
library(latex2exp)
k = 1:30
par(mar = c(4, 5.5, 1, 0.8)) 
plot(k, log10(1/k), type="l", lwd=5, col="green", 
     xlab = "Number of iterations", ylab = TeX("$||x_n - x^*||_2$"), 
     cex.lab=2, cex.axis=2, cex=0.7)
lines(k, log10((7/10)^k), col="red", lwd=5)
lines(k, log10((1/k)^(2^k)), col="blue", lwd=5)
legend(20, -0.1, legend=c("Sublinear", "Linear", "Superlinear"), fill=c("green", "red", "blue"), cex=1.5)
```

&lt;img src="Lecture8_Numerical_Analysis_files/figure-html/unnamed-chunk-19-1.png" style="display: block; margin: auto;" /&gt;

---

# Rate of Convergence for Functions

Suppose that `\(\lim\limits_{h\to 0} g(h)=0\)` with `\(g(h)&gt;0\)` for `\(h\in \mathbb{R}\)` and `\(\lim\limits_{h\to 0} f(h)=L\)`. If there exists a constant `\(C&gt;0\)` such that
`$$|f(h) -L| \leq C|g(h)| \quad \text{ for sufficiently small } |h|,$$`
then we write `\(|f(h)-L|=O(g(h))\)` or `\(f(h)=L+O(g(h))\)`. 

--

- Generally, `\(g(h)\)` has the form `\(g(h)=|h|^p\)` for some `\(p&gt;0\)`.

- We are interested in the largest value of `\(p&gt;0\)` for which `\(f(h) = L+ O(|h|^p)\)`.

Note: Other definitions of big-O and little-o notations can be found in the [Wikipedia page](https://en.wikipedia.org/wiki/Big_O_notation).

---
class: inverse

# Part 4: Solutions of Equations in One Variable

---

# Root-Finding Problems

One of the most basic problems of numerical analysis is the **root-finding** problem.

- Given a function `\(f:\mathbb{R}^{d_1} \to \mathbb{R}^{d_2}\)`, we want to solve the equation `\(f(x)=0\)`.

--

- A root of this equation `\(f(x)=0\)` is also called a **zero** of the function `\(f\)`.

- In most cases, we will focus on the cases where `\(d_1=d_2=1\)`.

--

- The root-finding methods that we will discuss include
  
  - Bisection method;
  
  - Fixed-point iteration;
  
  - Newton's method.
  
---

# Bisection Method

Suppose that `\(f\)` is a _continuous_ function defined on `\([a,b]\)` with `\(f(a)\)` and `\(f(b)\)` of opposite sign.

- The Intermediate Value Theorem implies that there exists a number `\(x^*\in (a,b)\)` such that `\(f(x^*)=0\)`.

- Note that the solution `\(x^*\)` to `\(f(x)=0\)` needs not be unique.

&lt;p align="center"&gt;
&lt;img src="./figures/bisection.png" width="600"/&gt;
&lt;/p&gt;

---

# Bisection Method

The **bisection method** uses the following procedure to approximate a solution to `\(f(x)=0\)`.

1. Initialize the endpoints `\(L=a, U=b\)`, and a tolerance level `\(\epsilon&gt;0\)`.

--

2. Compute the midpoint of the current interval `\([L,U]\)` as `\(p_n=\frac{L+U}{2}\)` for the `\(n^{th}\)` iteration.

--

  - If `\(f(p_n)=0\)`, then `\(x^*=p\)`, and we are done.
  
  - If `\(f(p_n)\)` and `\(f(L)\)` have the same sign, then set `\(L=p_n\)` and `\(U=b\)`.
  
  - If `\(f(p_n)\)` and `\(f(U)\)` have the same sign, then set `\(U=p_n\)` and `\(L=a\)`.
  
3. Repeat Step 2 until `\(|U-L| &lt; \epsilon\)`. Then, use `\(p_n=\frac{L+U}{2}\)` as the approximated solution.

---

# Bisection Method (Example)

Verify that the function `\(f(x)=x^3+7x^2-10=0\)` has at least one root in `\([1,2]\)`, and use the bisection method to find an approximated root that has an accuracy within `\(10^{-7}\)`.

--

- `\(f\)` is continuous with `\(f(1)=-2\)` and `\(f(2)=26\)`.


```r
f = function(x) { x^3 + 7*x^2 - 10 }
biSect = function(L, U, fun, tol=1e-7) {
  while(abs(L-U) &gt; tol) {
    p = (U + L)/2
    if(f(p) &gt; 0) {
      U = p
    } else if (fun(p) &lt; 0) {
      L = p
    } else {
      break
    }
  }
  return(p)
}
x_app = biSect(L = 1, U = 2, fun = f, tol = 1e-7)
cat("The approximated solution is ", x_app, ".", sep = "")
```

```
## The approximated solution is 1.110399.
```

```r
f(x_app)
```

```
## [1] 4.891655e-08
```

---

# Rate of Convergence for the Bisection Method

Let `\([a,b]\)` be the initial interval for the bisection method and `\(p_n\)` be the midpoint of the interval in the `\(n^{th}\)` iteration.

--

- Then, the difference between `\(p_n\)` and a solution `\(x^*\)` is upper bounded by
`$$\|p_n -x^* \|_2 \leq \frac{b-a}{2^n}.$$`

- Hence, the bisection method produce a sequence `\(\{p_n\}_{n=1}^{\infty}\)` that _converges (R-)linearly_ to the solution `\(x^*\)`.

---

# Caveats About the Bisection Method

- The _continuity_ of the function `\(f\)` within the interval of interest is critical for finding the root of `\(f(x)=0\)`.

&lt;p align="center"&gt;
&lt;img src="./figures/1_x.png" width="500"/&gt;
&lt;/p&gt;

---

# Caveats About the Bisection Method

- The choice of the initial interval can affect the convergence speed and the output approximated solution as well.


```r
x = seq(-8, 3, by = 0.1)
par(mar = c(4, 5.5, 1, 0.8)) 
plot(x, f(x), lwd = 5, type = "l")
abline(h = 0, col="red", lty=2, lwd = 5)
legend(-3, -50, legend=c(TeX("$f(x)=x^3+7x^2-10$")), fill=c("black"), cex=1.5)
```

&lt;img src="Lecture8_Numerical_Analysis_files/figure-html/unnamed-chunk-21-1.png" style="display: block; margin: auto;" /&gt;

---

# Caveats About the Bisection Method

&lt;img src="Lecture8_Numerical_Analysis_files/figure-html/unnamed-chunk-22-1.png" style="display: block; margin: auto;" /&gt;


```r
x_app = biSect(L = 1, U = 2, fun = f, tol = 1e-7)
cat("The approximated solution is ", x_app, ".", sep = "")
```

```
## The approximated solution is 1.110399.
```

```r
x_app = biSect(L = -8, U = 3, fun = f, tol = 1e-7)
cat("The approximated solution is ", x_app, ".", sep = "")
```

```
## The approximated solution is -6.782628.
```

---

# More General Binary-Search Method

Suppose that we want to search for an item in a (discrete) ordered list or a solution to a problem within an interval.

&lt;p align="center"&gt;
&lt;img src="./figures/Binary_Search.png" width="480"/&gt;
&lt;/p&gt;

--

- The two endpoints of the ordered list or the interval characterize two different states with respect to the target. (Recall that `\(f\in \mathcal{C}[a,b]\)` has two different signs at `\(a\)` and `\(b\)`.)

--

- We can also determine the state of the midpoint and update the search interval accordingly.

--

- Keep dividing the ordered list or interval into a half until the target item is found or reaching the tolerance level.

---

# Searching for `\(R_c\)` in the Final Project

We will write a function `findRc()` in the [final project](https://zhangyk8.github.io/teaching/file_stat302/Lectures/Final_Project.pdf) to search for `\(R_c\)`, which is the smallest possible radius such that a given ad hoc network is completely connected.

1. Denote the endpoints of the interval returned from `findRange()` (i.e., a helper function in the [final project](https://zhangyk8.github.io/teaching/file_stat302/Lectures/Final_Project.pdf)) by `\(\bar{R}_{\min}\)` and `\(\bar{R}_{\max}\)`, respectively.

2. Compute the middle point as `\(\bar{R}_0=\frac{\bar{R}_{\min}+\bar{R}_{\max}}{2}\)`. 

3. If `\(\bar{R}_0\)` gives a completely connected network, then we assign `\(\bar{R}_{\max}\gets \bar{R}_0\)`. Otherwise, we assign `\(\bar{R}_{\min}\gets \bar{R}_0\)`.
	
4. Repeat Steps 2 and 3 until `\(|\bar{R}_{\min} - \bar{R}_{\max}|\)` is less than the tolerance level `tol`.

---

# Fixed Points and Root-Finding

The point `\(p\in \mathbb{R}^d\)` is a **fixed point** for a given function `\(g\)` if `\(g(p) =p\)`.

- Given a root-finding problem `\(f(p)=0\)`, we can define the function `\(g\)` with a fixed point at `\(p\)` in various ways:
`$$g(x) = x-f(x), \quad g(x)=x+3f(x), \quad \text{etc.}$$`
Both of them satisfy `\(g(p)=p\)`.

- Conversely, if the function `\(g\)` has a fixed point at `\(p\)`, then the function defined by 
`$$f(x)=x-g(x)$$`
has a zero at `\(p\)`.

---

# Fixed Points and Root-Finding

Example: Determine any fixed points of the function `\(g(x)=x^2-2\)`.

--

&lt;p align="center"&gt;
&lt;img src="./figures/fixed_point.png" width="700"/&gt;
&lt;/p&gt;

- The function `\(g(x)=x^2-2\)` has two fixed points `\(x=-1\)` and `\(x=2\)`.

---

# Existence and Uniqueness of Fixed Points

**Theorem.** 

1. (_Existence_) If `\(g\in \mathcal{C}[a,b]\)` and `\(g(x) \in [a,b]\)` for all `\(x\in [a,b]\)`, then `\(g\)` has at least one fixed point in `\([a,b]\)`.

2. (_Uniqueness_) If, in addition, `\(g'(x)\)` exists on `\((a,b)\)` and there exists a positive constant `\(k&lt;1\)` such that
`$$|g'(x)| \leq k \quad \text{ for all } x\in (a,b),$$`
then there exists exactly one fixed point in `\([a,b]\)`.

&lt;p align="center"&gt;
&lt;img src="./figures/fixed_point_uni.png" width="500"/&gt;
&lt;/p&gt;

More general results for the uniqueness of the fixed point can be found as [Banach fixed-point theorem](https://en.wikipedia.org/wiki/Banach_fixed-point_theorem) and [this paper](https://www.ams.org/journals/proc/1976-060-01/S0002-9939-1976-0423137-6/S0002-9939-1976-0423137-6.pdf).

---

# Fixed-Point Iteration

The **fixed-point iteration** solves for the fixed point `\(p\)` (with `\(p=g(p)\)`) via the following procedure:

1. Choose an initial approximation `\(p_0\)`;

2. Generate the sequence `\(\{p_n\}_{n=0}^{\infty}\)` by letting 
`$$p_n=g(p_{n-1}) \quad \text{ for each } n\geq 1.$$`

If the sequence `\(\{p_n\}_{n=0}^{\infty}\)` converges to `\(p\)` and `\(g\)` is continuous, then
`$$p=\lim_{n\to\infty} p_n = \lim_{n\to\infty} g(p_{n-1})= g\left(\lim_{n\to\infty} p_{n-1} \right) = g(p).$$`

&lt;p align="center"&gt;
&lt;img src="./figures/fixed_point_iter.png" width="600"/&gt;
&lt;/p&gt;

---

# Convergence of Fixed-Point Iteration

**Theorem.** Let `\(g\in \mathcal{C}[a,b]\)` be such that `\(g(x)\in [a,b]\)` for all `\(x\in [a,b]\)`. Suppose, in addition, that `\(g'\)` exists on `\((a,b)\)` and that a constant `\(0&lt;k&lt;1\)` exists with
`$$|g'(x)| \leq k \quad \text{ for all } x\in (a,b).$$`
Then, for any number `\(p_0\in [a,b]\)`, the sequence `\(\{p_n\}_{n=0}^{\infty}\)` defined by 
`$$p_n=g(p_{n-1}), \quad n\geq 1,$$`
converges to the unique fixed point `\(p\in [a,b]\)`.

**Corollary.** 

---

# Fixed-Point Iteration (Example)

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "tomorrow-night-bright",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
